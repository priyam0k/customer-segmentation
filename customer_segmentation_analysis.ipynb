{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Project Setup and Data Ingestion\n",
    "Our first step is to set up the environment by importing the necessary Python libraries. We'll use `pandas` and `numpy` for data manipulation, `matplotlib` and `seaborn` for static visualizations, and `plotly` for interactive plots. We also set some default styles for our plots to ensure they are clear and aesthetically pleasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('seaborn-v0_8-talk')\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the libraries loaded, we can now ingest our dataset. For this project, we are using a publicly available, anonymized insurance dataset. We'll load this directly into a pandas DataFrame from a new, stable URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Corrected URL for the dataset\n",
    "    url = 'https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv'\n",
    "    df = pd.read_csv(url)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Cleaning and Initial Inspection\n",
    "Before diving into analysis, it's crucial to get a high-level overview of the data. We'll use `.head()` to see the first few rows, `.info()` to understand the data types and non-null counts, and `.describe()` to get a statistical summary of the numerical columns. This helps us quickly identify the structure and scale of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- First 5 Rows of the Dataset ---\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n--- Dataset Information ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- Statistical Summary ---\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data quality is paramount for any reliable analysis. A key step in data cleaning is checking for missing values. We'll use `.isnull().sum()` to count the number of nulls in each column. Fortunately, this dataset is also clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Missing Values per Column ---\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Exploratory Data Analysis (EDA)\n",
    "Now we move to Exploratory Data Analysis (EDA). The first step is to understand the distribution of our key numerical features. By plotting histograms for attributes like Age, BMI, and Charges, we can visualize their shape, central tendency, and spread, which provides initial insights into our customer base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting Exploratory Data Analysis ---\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "fig.suptitle('Distribution of Key Customer Attributes', fontsize=20)\n",
    "\n",
    "sns.histplot(df['age'], kde=True, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Age Distribution')\n",
    "\n",
    "sns.histplot(df['bmi'], kde=True, ax=axes[1], color='salmon')\n",
    "axes[1].set_title('BMI Distribution')\n",
    "\n",
    "sns.histplot(df['charges'], kde=True, ax=axes[2], color='lightgreen')\n",
    "axes[2].set_title('Insurance Charges Distribution')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how our numerical variables relate to one another, we'll create a correlation heatmap. This visualization shows the correlation coefficient between each pair of features. It's a powerful tool for identifying interesting relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "numeric_df = df.select_dtypes(include=np.number)\n",
    "correlation_matrix = numeric_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Feature Engineering & Preprocessing\n",
    "Machine learning models require all input features to be numerical. Our dataset contains categorical columns (`sex`, `smoker`, `region`) that we need to convert. We will use **one-hot encoding** for this, which creates new binary (0 or 1) columns for each category.\n",
    "\n",
    "Additionally, distance-based algorithms like K-Means are sensitive to the scale of the data. Features with large ranges (like `charges`) can dominate the clustering process. To prevent this, we will standardize all our numerical features using `StandardScaler` to give them a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "df_processed = pd.get_dummies(df_processed, columns=['sex', 'smoker', 'region'], drop_first=True)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_processed)\n",
    "\n",
    "print(\"Data preprocessed and scaled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Dimensionality Reduction with PCA\n",
    "Our preprocessed data now has many dimensions (features), which can be difficult to visualize and may contain redundant information. We will use **Principal Component Analysis (PCA)** to reduce the dimensionality. PCA transforms the data into a smaller set of uncorrelated variables called 'principal components' while retaining most of the original information. For this project, we'll reduce the data to two principal components, which will allow us to easily plot and visualize the customer clusters in a 2D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(\"PCA completed. Data reduced to 2 components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Model Development - K-Means Clustering\n",
    "With our data prepared, we can now apply the K-Means clustering algorithm. A critical step in K-Means is to determine the optimal number of clusters, `k`. We will use the **Elbow Method** for this.\n",
    "\n",
    "The Elbow Method involves running the K-Means algorithm for a range of `k` values and calculating the Within-Cluster Sum of Squares (WCSS) for each. WCSS is the sum of the squared distances between each data point and its assigned cluster's center. We then plot these WCSS values against `k`. The 'elbow' on the plot—the point where the rate of decrease in WCSS slows down significantly—is considered a good estimate for the optimal `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Use the Elbow Method to find the optimal number of clusters\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(df_pca)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Method graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Final Model Training and Visualization\n",
    "The elbow plot suggests that the optimal number of clusters is around 3 or 4. We will choose `k=4` to potentially capture more granular segments. Now, we'll train the final K-Means model with our chosen `k` and assign a cluster label to each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimal number of clusters\n",
    "optimal_k = 4\n",
    "\n",
    "# Train the final K-Means model\n",
    "kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(df_pca)\n",
    "\n",
    "# Add the cluster labels to our PCA dataframe\n",
    "df_pca['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"Final model trained with {optimal_k} clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cluster labels assigned, we can now visualize the segments. We will create a scatter plot of the two principal components, with each point colored according to its assigned cluster. This plot provides a clear, intuitive view of how the customer groups are separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=df_pca, palette='viridis', s=100, alpha=0.8)\n",
    "plt.title('Customer Segments Visualized with PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Customer Segment')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Cluster Profiling and Interpretation\n",
    "The final and most critical step is to understand what defines each cluster. We will add the cluster labels back to our original, unscaled DataFrame. Then, by grouping the data by cluster and calculating the average value for each feature, we can create a detailed profile for each segment. This analysis is what transforms the abstract clusters into actionable business personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels back to the original dataframe\n",
    "df['Cluster'] = cluster_labels\n",
    "\n",
    "# Analyze the characteristics of each cluster\n",
    "cluster_profile = df.groupby('Cluster').mean()\n",
    "\n",
    "print(\"\\n--- Customer Segment Profiles ---\")\n",
    "print(cluster_profile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
