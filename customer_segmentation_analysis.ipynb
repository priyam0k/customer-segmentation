{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Project Setup and Data Ingestion\n",
    "Our first step is to set up the environment by importing the necessary Python libraries. We'll use `pandas` and `numpy` for data manipulation, `matplotlib` and `seaborn` for static visualizations, and `plotly` for interactive plots. We also set some default styles for our plots to ensure they are clear and aesthetically pleasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('seaborn-v0_8-talk')\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Ingestion & Cleaning\n",
    "With the libraries loaded, we can now ingest our dataset. We are using a publicly available, anonymized insurance dataset. We'll load this directly into a pandas DataFrame, check its structure, and verify that there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    url = 'https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv'\n",
    "    df = pd.read_csv(url)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "\n",
    "print(\"\\n--- First 5 Rows ---\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n--- Data Info ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- Missing Values Check ---\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Exploratory Data Analysis (EDA)\n",
    "Now we move to Exploratory Data Analysis (EDA). We'll start by understanding the distribution of our key numerical features (Age, BMI, Charges) to visualize their shape, central tendency, and spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "fig.suptitle('Distribution of Key Customer Attributes', fontsize=20)\n",
    "\n",
    "sns.histplot(df['age'], kde=True, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Age Distribution')\n",
    "\n",
    "sns.histplot(df['bmi'], kde=True, ax=axes[1], color='salmon')\n",
    "axes[1].set_title('BMI Distribution')\n",
    "\n",
    "sns.histplot(df['charges'], kde=True, ax=axes[2], color='lightgreen')\n",
    "axes[2].set_title('Insurance Charges Distribution')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how our numerical variables relate to one another, we'll create a correlation heatmap and a pair plot. The heatmap gives a quick overview of linear relationships, while the pair plot allows us to see both distributions and relationships between pairs of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=np.number)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = numeric_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "sns.pairplot(df, hue='smoker', palette='viridis')\n",
    "plt.suptitle('Pair Plot of Insurance Data', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Feature Engineering & Preprocessing\n",
    "While the current dataset doesn't support creating features like `premium-to-income ratio`, we can still engineer valuable new features. We'll create `age_group` and `bmi_category` to turn continuous variables into categorical ones, which can help the model find clearer boundaries. After engineering, we'll perform one-hot encoding on all categorical variables and then scale the entire dataset with `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "\n",
    "# Feature Engineering\n",
    "df_processed['age_group'] = pd.cut(df_processed['age'], bins=[17, 30, 50, 65], labels=['Young Adult', 'Adult', 'Senior'])\n",
    "df_processed['bmi_category'] = pd.cut(df_processed['bmi'], bins=[0, 18.5, 24.9, 29.9, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
    "\n",
    "# One-hot encode all categorical features\n",
    "df_processed = pd.get_dummies(df_processed, columns=['sex', 'smoker', 'region', 'age_group', 'bmi_category'], drop_first=True)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_processed)\n",
    "\n",
    "print(\"Feature engineering and preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Dimensionality Reduction (PCA)\n",
    "To enable effective 2D visualization and reduce noise, we'll apply Principal Component Analysis (PCA) to reduce our high-dimensional dataset to just two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "print(\"PCA completed. Data reduced to 2 components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Clustering Model Selection & Training\n",
    "A critical step in K-Means is determining the optimal number of clusters, `k`. We will use a dual-validation approach:\n",
    "1. **The Elbow Method**: To find the point of diminishing returns in WCSS.\n",
    "2. **Silhouette Score**: To measure how well-separated the clusters are. A higher score is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(df_pca)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(df_pca, kmeans.labels_))\n",
    "\n",
    "# Plotting the results\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "ax1.plot(k_range, wcss, marker='o', linestyle='--', color='b')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('WCSS', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax1.set_title('Elbow Method & Silhouette Score for Optimal k')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(k_range, silhouette_scores, marker='s', linestyle='-', color='r')\n",
    "ax2.set_ylabel('Silhouette Score', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elbow Method shows a clear elbow at `k=4`, after which the decrease in WCSS slows. The Silhouette Score is also highest at `k=4`. Therefore, we will choose **4 clusters** as the optimal number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 (Alternative) Hierarchical Clustering\n",
    "As an alternative to K-Means, we can use hierarchical clustering to build a tree of clusters, visualized as a dendrogram. This helps us see the nested grouping of data points at different scales and can provide another perspective on the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram = sch.dendrogram(sch.linkage(df_pca, method='ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean Distances')\n",
    "plt.axhline(y=15, color='r', linestyle='--') # Cutoff line suggestion\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendrogram also suggests that cutting the tree to form 4 clusters (as indicated by the red line crossing four vertical lines) is a reasonable choice, reinforcing our conclusion from the K-Means validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Cluster Analysis & Profiling\n",
    "Now we train our final K-Means model with `k=4`, assign the cluster labels back to the original dataset, and then group by these labels to analyze the average characteristics of each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(df_pca)\n",
    "\n",
    "# Add cluster labels back to the original and PCA dataframes\n",
    "df['Cluster'] = cluster_labels\n",
    "df_pca['Cluster'] = cluster_labels\n",
    "\n",
    "# Analyze the characteristics of each cluster\n",
    "cluster_profile = df.groupby('Cluster').mean(numeric_only=True)\n",
    "\n",
    "print(\"\\n--- Customer Segment Profiles ---\")\n",
    "print(cluster_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Persona Creation\n",
    "Based on the cluster profiles, we can create detailed personas:\n",
    "- **Cluster 0: Healthy & Low-Cost Non-Smokers**: This is the largest group. They are non-smokers with a healthy BMI and by far the lowest average insurance charges. They represent a low-risk, high-value customer base.\n",
    "- **Cluster 1: High-Risk, High-Cost Smokers**: This group has the highest average insurance charges. They are almost exclusively smokers, are older, and have a higher BMI. They are a high-cost, high-risk segment.\n",
    "- **Cluster 2: Mid-Risk Families**: This segment has moderate insurance charges. They are older, have a higher BMI, and more children on average, but are non-smokers. They represent a stable, predictable family-oriented group.\n",
    "- **Cluster 3: Young & Healthy Individuals**: This group consists of the youngest customers with a healthy BMI and few children. Their insurance charges are low, making them a key target for long-term customer value and future cross-selling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Visualization & Interpretation\n",
    "To clearly illustrate the differences between segments, we'll create several compelling visualizations: an interactive scatter plot, bar charts comparing key features, and a radar chart for a holistic persona comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add original data to the PCA dataframe for rich hover information\n",
    "df_pca['age'] = df['age']\n",
    "df_pca['bmi'] = df['bmi']\n",
    "df_pca['charges'] = df['charges']\n",
    "df_pca['smoker'] = df['smoker']\n",
    "\n",
    "# Interactive Scatter Plot\n",
    "fig = px.scatter(df_pca, x='PC1', y='PC2', color='Cluster', \n",
    "                 title='Interactive Customer Segments Visualization', \n",
    "                 hover_data=['age', 'bmi', 'charges', 'smoker'],\n",
    "                 color_continuous_scale='viridis')\n",
    "fig.show()\n",
    "\n",
    "# Bar Charts for Feature Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "fig.suptitle('Comparison of Key Features Across Segments', fontsize=20)\n",
    "sns.barplot(data=df, x='Cluster', y='charges', ax=axes[0])\n",
    "axes[0].set_title('Average Charges')\n",
    "sns.barplot(data=df, x='Cluster', y='age', ax=axes[1])\n",
    "axes[1].set_title('Average Age')\n",
    "sns.barplot(data=df, x='Cluster', y='bmi', ax=axes[2])\n",
    "axes[2].set_title('Average BMI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar Chart for Persona Comparison\n",
    "profile_scaled = scaler.fit_transform(cluster_profile)\n",
    "profile_scaled = pd.DataFrame(profile_scaled, index=cluster_profile.index, columns=cluster_profile.columns)\n",
    "\n",
    "categories = list(profile_scaled.columns)\n",
    "N = len(categories)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(profile_scaled)):\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=profile_scaled.iloc[i].values,\n",
    "        theta=categories,\n",
    "        fill='toself',\n",
    "        name=f'Cluster {i}'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "      range=[-1.5, 2.5]\n",
    "    )),\n",
    "  showlegend=True,\n",
    "  title='Radar Chart of Scaled Persona Characteristics'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
